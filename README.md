<div align="center">

```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— 
â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•      â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â• 
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â•šâ–ˆâ–ˆâ•”â•     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•      â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•â•   â•šâ•â•      â•šâ•â•   â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•â• â•šâ•â•â•â•â•â• 
```

<div style="margin: 20px 0;">
  <img src="https://readme-typing-svg.herokuapp.com?font=Orbitron&size=35&duration=3000&pause=1000&color=00F5FF&background=0D001100&center=true&vCenter=true&width=1000&height=100&lines=ğŸ”¥+NEXT-GEN+MULTIMODAL+RAG+SYSTEM+ğŸ”¥;âš¡+CYBER-ENHANCED+AI+INTELLIGENCE+âš¡;ğŸš€+QUANTUM+LEAP+IN+DOCUMENT+PROCESSING+ğŸš€" alt="Typing SVG" />
</div>

<div style="background: linear-gradient(45deg, #FF0080, #00F5FF); padding: 2px; border-radius: 15px; margin: 20px 0;">
  <div style="background: #0a0a0a; padding: 20px; border-radius: 13px;">
    <img src="./assets/logo.png" width="120" height="120" alt="raganything" style="border-radius: 50%; border: 3px solid #00F5FF; box-shadow: 0 0 30px #00F5FF;">
  </div>
</div>

<div style="display: flex; justify-content: center; gap: 10px; flex-wrap: wrap; margin: 20px 0;">
  <a href='https://github.com/HKUDS/RAG-Anything'>
    <img src='https://img.shields.io/badge/ğŸ”¥_PROJECT-NEXUS-FF0080?style=for-the-badge&logo=github&logoColor=white&labelColor=0a0a0a' alt="Project">
  </a>
  <a href='https://arxiv.org/abs/2410.05779'>
    <img src='https://img.shields.io/badge/ğŸ“š_ARXIV-2410.05779-00F5FF?style=for-the-badge&logo=arxiv&logoColor=white&labelColor=0a0a0a' alt="arXiv">
  </a>
  <a href='https://github.com/HKUDS/LightRAG'>
    <img src='https://img.shields.io/badge/âš¡_POWERED_BY-LIGHTRAG-39FF14?style=for-the-badge&logo=lightning&logoColor=white&labelColor=0a0a0a' alt="LightRAG">
  </a>
</div>

<div style="display: flex; justify-content: center; gap: 10px; flex-wrap: wrap; margin: 20px 0;">
  <img src='https://img.shields.io/github/stars/HKUDS/RAG-Anything?color=FF0080&style=for-the-badge&logo=star&logoColor=white&labelColor=0a0a0a' alt="Stars">
  <img src="https://img.shields.io/badge/PYTHON-3.9+-00F5FF?style=for-the-badge&logo=python&logoColor=white&labelColor=0a0a0a" alt="Python">
  <img src="https://img.shields.io/pypi/v/raganything.svg?color=39FF14&style=for-the-badge&logo=pypi&logoColor=white&labelColor=0a0a0a" alt="PyPI">
</div>

<div style="margin: 30px 0;">
  <a href="README_zh.md" style="text-decoration: none;">
    <img src="https://img.shields.io/badge/ğŸ‡¨ğŸ‡³_ä¸­æ–‡ç‰ˆ-FF6B35?style=for-the-badge&logoColor=white&labelColor=0a0a0a" alt="Chinese">
  </a>
  <span style="color: #666; margin: 0 10px;">|</span>
  <a href="README.md" style="text-decoration: none;">
    <img src="https://img.shields.io/badge/ğŸ‡ºğŸ‡¸_ENGLISH-4CAF50?style=for-the-badge&logoColor=white&labelColor=0a0a0a" alt="English">
  </a>
</div>

</div>

---

## ğŸŒ€ **NEURAL GENESIS** 

<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&height=200&color=gradient&customColorList=12,20,3&text=MULTIMODAL%20AI%20REVOLUTION&fontSize=40&fontColor=ffffff&animation=twinkling" />
</div>

In the **cybernetic landscape** of modern AI, documents have evolved beyond mere text into **multi-dimensional data matrices** containing interleaved visual intelligence, structured knowledge graphs, and mathematical abstractions. Traditional RAG systems, trapped in their **monochrome reality**, cannot perceive this rich multimodal spectrum.

**RAG-Anything** emerges as the **quantum leap** in AI document processingâ€”a **neural-enhanced, cyberpunk-grade** multimodal RAG system that **shatters conventional boundaries**. Built on the lightning-fast [LightRAG](https://github.com/HKUDS/LightRAG) architecture, it transforms the impossible into inevitable.

<div align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Orbitron&size=25&duration=2000&pause=500&color=FF0080&center=true&vCenter=true&width=800&lines=ğŸ”®+UNIFIED+MULTIMODAL+INTELLIGENCE;ğŸ’+SEAMLESS+CROSS-MODAL+FUSION;ğŸ›¸+ENTERPRISE-GRADE+PERFORMANCE;âš”ï¸+ZERO-COMPROMISE+ACCURACY" alt="Features" />
</div>

### ğŸ”¥ **CYBERNETIC CAPABILITIES**

<div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 30px 0;">

<div style="background: linear-gradient(135deg, #FF008080, #FF008020); padding: 20px; border-radius: 15px; border: 1px solid #FF0080;">
  <h4>ğŸš€ <strong>QUANTUM PIPELINE</strong></h4>
  <p>End-to-end multimodal workflow with <strong>neural-optimized</strong> document ingestion, parsing, and intelligent query synthesis</p>
</div>

<div style="background: linear-gradient(135deg, #00F5FF80, #00F5FF20); padding: 20px; border-radius: 15px; border: 1px solid #00F5FF;">
  <h4>ğŸ“¡ <strong>UNIVERSAL COMPATIBILITY</strong></h4>
  <p>Seamless processing across <strong>all known formats</strong>: PDFs, Office suites, images, and emerging data structures</p>
</div>

<div style="background: linear-gradient(135deg, #39FF1480, #39FF1420); padding: 20px; border-radius: 15px; border: 1px solid #39FF14;">
  <h4>ğŸ§  <strong>MULTI-MODAL COGNITION</strong></h4>
  <p>Specialized neural processors for images, tables, equations, and <strong>heterogeneous content matrices</strong></p>
</div>

<div style="background: linear-gradient(135deg, #FF6B3580, #FF6B3520); padding: 20px; border-radius: 15px; border: 1px solid #FF6B35;">
  <h4>ğŸ•¸ï¸ <strong>KNOWLEDGE NEXUS</strong></h4>
  <p>Self-assembling multimodal knowledge graphs with <strong>cross-dimensional relationship mapping</strong></p>
</div>

<div style="background: linear-gradient(135deg, #8A2BE280, #8A2BE220); padding: 20px; border-radius: 15px; border: 1px solid #8A2BE2;">
  <h4>âš¡ <strong>ADAPTIVE PROTOCOLS</strong></h4>
  <p>Flexible MinerU-powered parsing with <strong>real-time content injection</strong> workflows</p>
</div>

<div style="background: linear-gradient(135deg, #FF147580, #FF147520); padding: 20px; border-radius: 15px; border: 1px solid #FF1475;">
  <h4>ğŸ¯ <strong>HYBRID INTELLIGENCE</strong></h4>
  <p>Advanced retrieval spanning textual and multimodal domains with <strong>contextual consciousness</strong></p>
</div>

</div>

---

## ğŸ”¬ **NEURAL ARCHITECTURE MATRIX**

<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=rect&height=100&color=gradient&customColorList=30,12,20&text=ALGORITHMIC%20CORE&fontSize=30&fontColor=ffffff" />
</div>

**RAG-Anything** implements a **revolutionary multi-stage neural pipeline** that fundamentally transcends traditional RAG architectures, orchestrating seamless multimodal understanding through **quantum-enhanced content processing**.

### ğŸŒŠ **STAGE I: DOCUMENT DECONSTRUCTION**

<div style="background: linear-gradient(45deg, #0a0a0a, #1a1a1a); padding: 20px; border-radius: 15px; border-left: 5px solid #FF0080; margin: 20px 0;">

**âš™ï¸ MinerU Neural Integration**: Harnesses advanced [MinerU](https://github.com/opendatalab/MinerU) engines for **high-fidelity document archaeology** and semantic preservation across complex multi-dimensional layouts.

**ğŸ§© Adaptive Content Atomization**: Intelligently segments documents into **coherent information quanta**â€”text blocks, visual elements, structured matrices, mathematical expressionsâ€”while preserving **quantum entanglement** between contextual relationships.

**ğŸ“ Universal Format Transcendence**: Comprehensive handling of all known formats through **specialized neural parsers** with format-specific optimization algorithms.

</div>

### ğŸŒ€ **STAGE II: MULTIMODAL CONSCIOUSNESS**

<div style="background: linear-gradient(45deg, #0a0a0a, #1a1a1a); padding: 20px; border-radius: 15px; border-left: 5px solid #00F5FF; margin: 20px 0;">

**ğŸ¯ Autonomous Content Classification**: AI-powered categorization and routing through **optimized neural pathways**

**âš¡ Concurrent Multi-Pipeline Architecture**: Parallel execution of textual and multimodal processing with **maximum throughput efficiency**

**ğŸ—ï¸ Document Hierarchy Extraction**: Preserves original structural DNA during **quantum content transformation**

</div>

### ğŸš€ **STAGE III: MULTIMODAL ANALYSIS ENGINE**

<div style="background: linear-gradient(45deg, #0a0a0a, #1a1a1a); padding: 20px; border-radius: 15px; border-left: 5px solid #39FF14; margin: 20px 0;">

<div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 15px; margin: 20px 0;">

<div style="border: 1px solid #FF0080; padding: 15px; border-radius: 10px;">
  <h4>ğŸ” <strong>VISUAL NEURAL CORTEX</strong></h4>
  <ul>
    <li>Advanced vision model integration</li>
    <li><strong>Context-aware semantic captioning</strong></li>
    <li>Spatial relationship extraction</li>
  </ul>
</div>

<div style="border: 1px solid #00F5FF; padding: 15px; border-radius: 10px;">
  <h4>ğŸ“Š <strong>STRUCTURED DATA DECODER</strong></h4>
  <ul>
    <li>Systematic tabular interpretation</li>
    <li><strong>Statistical pattern recognition</strong></li>
    <li>Cross-dataset dependency mapping</li>
  </ul>
</div>

<div style="border: 1px solid #39FF14; padding: 15px; border-radius: 10px;">
  <h4>ğŸ“ <strong>MATHEMATICAL PARSER</strong></h4>
  <ul>
    <li>Complex expression parsing</li>
    <li><strong>Native LaTeX support</strong></li>
    <li>Conceptual knowledge mapping</li>
  </ul>
</div>

<div style="border: 1px solid #FF6B35; padding: 15px; border-radius: 10px;">
  <h4>ğŸ”§ <strong>EXTENSIBLE MODALITY HANDLER</strong></h4>
  <ul>
    <li>Plugin architecture framework</li>
    <li><strong>Runtime pipeline configuration</strong></li>
    <li>Custom modality integration</li>
  </ul>
</div>

</div>

</div>

### ğŸ•¸ï¸ **STAGE IV: KNOWLEDGE GRAPH GENESIS**

<div style="background: linear-gradient(45deg, #0a0a0a, #1a1a1a); padding: 20px; border-radius: 15px; border-left: 5px solid #8A2BE2; margin: 20px 0;">

**ğŸ” Multi-Modal Entity Synthesis**: Transform significant multimodal elements into **structured knowledge graph entities** with semantic annotations

**ğŸ”— Cross-Modal Relationship Weaving**: Establish **semantic neural networks** between textual and multimodal components through automated inference

**ğŸ—ï¸ Hierarchical Structure Preservation**: Maintain original document organization through **"belongs_to" quantum chains**

**âš–ï¸ Weighted Relationship Scoring**: Quantitative relevance scoring based on **semantic proximity algorithms**

</div>

### ğŸ¯ **STAGE V: MODALITY-AWARE RETRIEVAL**

<div style="background: linear-gradient(45deg, #0a0a0a, #1a1a1a); padding: 20px; border-radius: 15px; border-left: 5px solid #FF1475; margin: 20px 0;">

**ğŸ”€ Vector-Graph Fusion**: Hybrid similarity search with **graph traversal algorithms** for comprehensive content retrieval

**ğŸ“Š Modality-Aware Ranking**: Adaptive scoring mechanisms with **query-specific modality preferences**

**ğŸ”— Relational Coherence**: Maintains semantic relationships for **contextually integrated information delivery**

</div>

---

## âš¡ **NEURAL INITIATION PROTOCOL**

<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=shark&height=150&color=gradient&customColorList=12,20,3&text=QUANTUM%20INSTALLATION&fontSize=35&fontColor=ffffff&animation=twinkling" />
</div>

### ğŸ”¥ **INSTALLATION MATRIX**

<div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(400px, 1fr)); gap: 20px; margin: 30px 0;">

<div style="background: linear-gradient(135deg, #FF008080, #FF008020); padding: 25px; border-radius: 15px; border: 2px solid #FF0080;">
  <h4>ğŸš€ <strong>OPTION Î±: PyPI NEXUS (RECOMMENDED)</strong></h4>
  
```bash
# Neural link establishment
pip install raganything
```
</div>

<div style="background: linear-gradient(135deg, #00F5FF80, #00F5FF20); padding: 25px; border-radius: 15px; border: 2px solid #00F5FF;">
  <h4>ğŸ›¸ <strong>OPTION Î²: SOURCE MATRIX</strong></h4>
  
```bash
# Clone the neural repository
git clone https://github.com/HKUDS/RAG-Anything.git
cd RAGAnything
pip install -e .
```
</div>

</div>

### ğŸ”§ **MINERU CYBERNETIC ENHANCEMENT**

<div style="background: linear-gradient(45deg, #39FF1480, #39FF1420); padding: 25px; border-radius: 15px; border: 2px solid #39FF14; margin: 20px 0;">

```bash
# Install MinerU 2.0 Neural Engine
pip install -U 'mineru[core]'

# Or via UV hyperdrive (âš¡ ULTRA-FAST)
uv pip install -U 'mineru[core]'
```

</div>

<div style="background: linear-gradient(45deg, #FF6B3580, #FF6B3520); padding: 20px; border-radius: 15px; border: 1px solid #FF6B35; margin: 20px 0;">

> **âš ï¸ CRITICAL NEURAL UPDATES IN MINERU 2.0:**
> - Package designation: `magic-pdf` â†’ `mineru` 
> - LibreOffice integration deprecated (manual PDF conversion required)
> - Streamlined CLI interface via `mineru` command
> - Enhanced backend options with **quantum performance boost**

</div>

**Neural System Verification:**
```bash
# Verify neural link
mineru --version

# Diagnostic protocol
python -c "from raganything import RAGAnything; rag = RAGAnything(); print('âœ… Neural link established' if rag.check_mineru_installation() else 'âŒ Neural link compromised')"
```

---

## ğŸ’» **NEURAL DEPLOYMENT PROTOCOLS**

<div align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Orbitron&size=30&duration=2500&pause=500&color=00F5FF&center=true&vCenter=true&width=900&lines=ğŸ”®+DEPLOY+MULTIMODAL+INTELLIGENCE;âš¡+ACTIVATE+NEURAL+PROCESSING;ğŸš€+LAUNCH+QUANTUM+QUERIES" alt="Deployment" />
</div>

### ğŸŒŠ **COMPLETE NEURAL WORKFLOW**

<div style="background: linear-gradient(45deg, #0a0a0a, #1a1a1a); padding: 25px; border-radius: 15px; border: 2px solid #FF0080; margin: 20px 0;">

```python
import asyncio
from raganything import RAGAnything
from lightrag.llm.openai import openai_complete_if_cache, openai_embed

async def neural_initialization():
    # Initialize RAG-Anything Neural Core
    rag = RAGAnything(
        working_dir="./neural_storage",
        llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",  # Neural language model
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key="your-quantum-api-key",
            **kwargs,
        ),
        vision_model_func=lambda prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs: openai_complete_if_cache(
            "gpt-4o",  # Visual neural cortex
            "",
            system_prompt=None,
            history_messages=[],
            messages=[
                {"role": "system", "content": system_prompt} if system_prompt else None,
                {"role": "user", "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
                    }
                ]} if image_data else {"role": "user", "content": prompt}
            ],
            api_key="your-quantum-api-key",
            **kwargs,
        ) if image_data else openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key="your-quantum-api-key",
            **kwargs,
        ),
        embedding_func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",  # Neural embedding matrix
            api_key="your-quantum-api-key",
        ),
        embedding_dim=3072,
        max_token_size=8192
    )

    # Document Neural Processing
    await rag.process_document_complete(
        file_path="path/to/your/document.pdf",
        output_dir="./neural_output",
        parse_method="auto"
    )

    # Quantum Query Execution
    result = await rag.query_with_multimodal(
        "ğŸ” Analyze the key findings in figures and tables with cybernetic precision",
        mode="hybrid"
    )
    print(f"ğŸš€ Neural Response: {result}")

if __name__ == "__main__":
    asyncio.run(neural_initialization())
```

</div>

### ğŸ§  **DIRECT MULTIMODAL NEURAL PROCESSING**

<div style="background: linear-gradient(45deg, #0a0a0a, #1a1a1a); padding: 25px; border-radius: 15px; border: 2px solid #00F5FF; margin: 20px 0;">

```python
import asyncio
from lightrag import LightRAG
from raganything.modalprocessors import ImageModalProcessor, TableModalProcessor

async def multimodal_neural_processing():
    # Initialize LightRAG Neural Core
    rag = LightRAG(
        working_dir="./neural_storage",
        # ... your neural configurations
    )
    await rag.initialize_storages()

    # Image Neural Processor
    image_processor = ImageModalProcessor(
        lightrag=rag,
        modal_caption_func=your_vision_neural_func
    )

    # Neural image content matrix
    image_content = {
        "img_path": "path/to/neural_image.jpg",
        "img_caption": ["Figure 1: Quantum experimental results"],
        "img_footnote": ["Data acquired in neural timeline 2024"]
    }

    # Process through visual neural cortex
    description, entity_info = await image_processor.process_multimodal_content(
        modal_content=image_content,
        content_type="image",
        file_path="research_neural_paper.pdf",
        entity_name="Quantum Results Figure"
    )

    # Table Neural Processor
    table_processor = TableModalProcessor(
        lightrag=rag,
        modal_caption_func=your_llm_neural_func
    )

    # Neural table data matrix
    table_content = {
        "table_body": """
        | Neural Method | Accuracy | Quantum F1-Score |
        |---------------|----------|------------------|
        | RAG-Anything  | 95.2%    | 0.94            |
        | Legacy System | 87.3%    | 0.85            |
        """,
        "table_caption": ["Neural Performance Comparison"],
        "table_footnote": ["Results on quantum test dataset"]
    }

    # Process through structured data neural decoder
    description, entity_info = await table_processor.process_multimodal_content(
        modal_content=table_content,
        content_type="table",
        file_path="research_neural_paper.pdf",
        entity_name="Neural Performance Results Table"
    )

if __name__ == "__main__":
    asyncio.run(multimodal_neural_processing())
```

</div>

### ğŸ”¥ **BATCH NEURAL PROCESSING**

<div style="background: linear-gradient(45deg, #39FF1480, #39FF1420); padding: 20px; border-radius: 15px; border: 1px solid #39FF14; margin: 20px 0;">

```python
# Mass document neural processing
await rag.process_folder_complete(
    folder_path="./neural_documents",
    output_dir="./neural_output",
    file_extensions=[".pdf", ".docx", ".pptx"],
    recursive=True,
    max_workers=4  # Parallel neural processing
)
```

</div>

### ğŸš€ **QUERY NEURAL MODES**

<div style="background: linear-gradient(45deg, #8A2BE280, #8A2BE220); padding: 20px; border-radius: 15px; border: 1px solid #8A2BE2; margin: 20px 0;">

```python
# Different neural query protocols
result_hybrid = await rag.query_with_multimodal("Your quantum query", mode="hybrid")
result_local = await rag.query_with_multimodal("Your quantum query", mode="local")
result_global = await rag.query_with_multimodal("Your quantum query", mode="global")
```

</div>

---

## ğŸ”¬ **NEURAL LABORATORIES**

<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&height=120&color=gradient&customColorList=30,12,20&text=EXPERIMENTAL%20PROTOCOLS&fontSize=25&fontColor=ffffff" />
</div>

The `examples/` directory contains **cutting-edge experimental protocols**:

<div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 15px; margin: 20px 0;">

<div style="border: 1px solid #FF0080; padding: 15px; border-radius: 10px; background: linear-gradient(135deg, #FF008020, #FF008005);">
  <h4>ğŸš€ <strong>raganything_example.py</strong></h4>
  <p>Complete neural workflow with MinerU integration</p>
</div>

<div style="border: 1px solid #00F5FF; padding: 15px; border-radius: 10px; background: linear-gradient(135deg, #00F5FF20, #00F5FF05);">
  <h4>ğŸ§  <strong>modalprocessors_example.py</strong></h4>
  <p>Direct multimodal neural processing protocols</p>
</div>

<div style="border: 1px solid #39FF14; padding: 15px; border-radius: 10px; background: linear-gradient(135deg, #39FF1420, #39FF1405);">
  <h4>ğŸ“„ <strong>office_document_test.py</strong></h4>
  <p>Office document parsing via neural networks</p>
</div>

<div style="border: 1px solid #FF6B35; padding: 15px; border-radius: 10px; background: linear-gradient(135deg, #FF6B3520, #FF6B3505);">
  <h4>ğŸ–¼ï¸ <strong>image_format_test.py</strong></h4>
  <p>Image format neural processing protocols</p>
</div>

</div>

**Execute Neural Experiments:**
```bash
# Complete neural processing
python examples/raganything_example.py path/to/document.pdf --api-key YOUR_QUANTUM_KEY

# Direct neural modal processing
python examples/modalprocessors_example.py --api-key YOUR_QUANTUM_KEY

# Neural parsing tests (no API required)
python examples/office_document_test.py --file path/to/document.docx
python examples/image_format_test.py --file path/to/image.bmp
```

---

## ğŸ”§ **NEURAL SYSTEM CONFIGURATION**

<div align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Orbitron&size=25&duration=2000&pause=500&color=FF0080&center=true&vCenter=true&width=700&lines=âš™ï¸+QUANTUM+CONFIGURATION;ğŸ”‘+NEURAL+API+KEYS;ğŸ› ï¸+SYSTEM+OPTIMIZATION" alt="Config" />
</div>

### ğŸ”‘ **Neural Environment Matrix**

<div style="background: linear-gradient(45deg, #0a0a0a, #1a1a1a); padding: 20px; border-radius: 15px; border: 2px solid #FF0080; margin: 20px 0;">

Create `.env` neural configuration (refer to `.env.example`):
```bash
OPENAI_API_KEY=your_quantum_neural_api_key
OPENAI_BASE_URL=your_neural_base_url  # Optional quantum endpoint
```

</div>

### âš™ï¸ **MinerU Neural Configuration**

<div style="background: linear-gradient(45deg, #0a0a0a, #1a1a1a); padding: 20px; border-radius: 15px; border: 2px solid #00F5FF; margin: 20px 0;">

```bash
# MinerU 2.0 neural command protocols
mineru --help  # Display neural options

# Common neural configurations
mineru -p input.pdf -o neural_output -m auto      # Auto neural parsing
mineru -p input.pdf -o neural_output -m ocr       # OCR neural focus
mineru -p input.pdf -o neural_output -b pipeline --device cuda  # GPU acceleration
```

**RAGAnything Neural Parameters:**
```python
# Configure neural parsing behavior
await rag.process_document_complete(
    file_path="document.pdf",
    parse_method="auto",     # Neural parsing mode
    device="cuda",           # GPU neural acceleration
    backend="pipeline",      # Neural parsing backend
    lang="en"               # Language neural optimization
)
```

</div>

---

## ğŸŒ **SUPPORTED NEURAL FORMATS**

<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=rect&height=100&color=gradient&customColorList=12,20,3&text=UNIVERSAL%20COMPATIBILITY&fontSize=25&fontColor=ffffff" />
</div>

<div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 30px 0;">

<div style="background: linear-gradient(135deg, #FF008080, #FF008020); padding: 20px; border-radius: 15px; border: 2px solid #FF0080;">
  <h4>ğŸ“„ <strong>DOCUMENT NEURAL MATRICES</strong></h4>
  <ul>
    <li><strong>PDFs</strong>: Research papers, reports, presentations</li>
    <li><strong>Office Neural Docs</strong>: DOC, DOCX, PPT, PPTX, XLS, XLSX âš ï¸</li>
    <li><strong>Images</strong>: JPG, PNG, BMP, TIFF, GIF, WebP ğŸ“¸</li>
    <li><strong>Text Files</strong>: TXT, MD âš ï¸</li>
  </ul>
</div>

<div style="background: linear-gradient(135deg, #00F5FF80, #00F5FF20); padding: 20px; border-radius: 15px; border: 2px solid #00F5FF;">
  <h4>ğŸ§  <strong>MULTIMODAL NEURAL ELEMENTS</strong></h4>
  <ul>
    <li><strong>Visual Intelligence</strong>: Photos, diagrams, charts, screenshots</li>
    <li><strong>Structured Data</strong>: Tables, comparisons, statistical matrices</li>
    <li><strong>Mathematical Expressions</strong>: LaTeX formulas, equations</li>
    <li><strong>Custom Neural Content</strong>: Extensible processors</li>
  </ul>
</div>

</div>

---

## ğŸ“š **ACADEMIC NEURAL CITATION**

<div align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Orbitron&size=30&duration=3000&pause=1000&color=39FF14&center=true&vCenter=true&width=800&lines=ğŸ“+CITE+OUR+NEURAL+RESEARCH;ğŸ“–+ACADEMIC+RECOGNITION;ğŸ†+SCIENTIFIC+CONTRIBUTION" alt="Citation" />
</div>

<div style="background: linear-gradient(45deg, #0a0a0a, #1a1a1a); padding: 25px; border-radius: 15px; border: 2px solid #39FF14; margin: 20px 0;">

If you find **RAG-Anything** useful in your neural research, please cite our groundbreaking paper:

```bibtex
@article{guo2024lightrag,
  title={LightRAG: Simple and Fast Retrieval-Augmented Generation},
  author={Zirui Guo and Lianghao Xia and Yanhua Yu and Tu Ao and Chao Huang},
  year={2024},
  eprint={2410.05779},
  archivePrefix={arXiv},
  primaryClass={cs.IR}
}
```

</div>

---

## ğŸ”— **NEURAL NETWORK ECOSYSTEM**

<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&height=150&color=gradient&customColorList=30,20,12&text=CONNECTED%20INTELLIGENCE&fontSize=30&fontColor=ffffff&animation=twinkling" />
</div>

<div style="display: flex; justify-content: center; gap: 15px; flex-wrap: wrap; margin: 30px 0;">
  <a href="https://github.com/HKUDS/LightRAG">
    <img src="https://img.shields.io/badge/âš¡_LIGHTRAG-SIMPLE_FAST_RAG-FF0080?style=for-the-badge&logo=lightning&logoColor=white&labelColor=0a0a0a" alt="LightRAG">
  </a>
  <a href="https://github.com/HKUDS/VideoRAG">
    <img src="https://img.shields.io/badge/ğŸ¬_VIDEORAG-EXTREME_LONG_CONTEXT-00F5FF?style=for-the-badge&logo=video&logoColor=white&labelColor=0a0a0a" alt="VideoRAG">
  </a>
  <a href="https://github.com/HKUDS/MiniRAG">
    <img src="https://img.shields.io/badge/ğŸ”¬_MINIRAG-EXTREMELY_SIMPLE-39FF14?style=for-the-badge&logo=atom&logoColor=white&labelColor=0a0a0a" alt="MiniRAG">
  </a>
</div>

---

## â­ **NEURAL EVOLUTION TIMELINE**

<div align="center">
  <a href="https://star-history.com/#HKUDS/RAG-Anything&Date">
    <picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&type=Date&theme=dark" />
      <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&type=Date" />
      <img alt="Neural Evolution Chart" src="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&type=Date" style="border-radius: 15px; border: 2px solid #00F5FF;" />
    </picture>
  </a>
</div>

---

## ğŸ¤ **NEURAL CONTRIBUTORS**

<div align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Orbitron&size=25&duration=2000&pause=500&color=FF6B35&center=true&vCenter=true&width=600&lines=ğŸ§ +COLLECTIVE+INTELLIGENCE;ğŸ¤–+NEURAL+COLLABORATION;ğŸš€+COMMUNITY+DRIVEN" alt="Contributors" />
</div>

We thank all our **neural contributors** for their quantum contributions to the multimodal AI revolution.

<div align="center" style="margin: 30px 0;">
  <a href="https://github.com/HKUDS/RAG-Anything/graphs/contributors">
    <img src="https://contrib.rocks/image?repo=HKUDS/RAG-Anything" style="border-radius: 15px; border: 2px solid #FF6B35;" />
  </a>
</div>

---

<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&height=200&color=gradient&customColorList=12,20,3,30&text=JOIN%20THE%20NEURAL%20REVOLUTION&fontSize=35&fontColor=ffffff&animation=twinkling" />
</div>

<div align="center" style="margin: 40px 0;">
  <div style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;">
    <a href="https://github.com/HKUDS/RAG-Anything" style="text-decoration: none;">
      <img src="https://img.shields.io/badge/â­_STAR_NEURAL_REPO-FF0080?style=for-the-badge&logo=github&logoColor=white&labelColor=0a0a0a" alt="Star">
    </a>
    <a href="https://github.com/HKUDS/RAG-Anything/issues" style="text-decoration: none;">
      <img src="https://img.shields.io/badge/ğŸ›_REPORT_NEURAL_BUGS-00F5FF?style=for-the-badge&logo=bug&logoColor=white&labelColor=0a0a0a" alt="Issues">
    </a>
    <a href="https://github.com/HKUDS/RAG-Anything/discussions" style="text-decoration: none;">
      <img src="https://img.shields.io/badge/ğŸ’¬_NEURAL_DISCUSSIONS-39FF14?style=for-the-badge&logo=chat&logoColor=white&labelColor=0a0a0a" alt="Discussions">
    </a>
  </div>
</div>

<div align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Orbitron&size=20&duration=4000&pause=1000&color=8A2BE2&center=true&vCenter=true&width=1000&lines=ğŸŒŸ+THE+FUTURE+OF+MULTIMODAL+AI+IS+NOW+ğŸŒŸ;âš¡+TRANSCEND+TRADITIONAL+BOUNDARIES+âš¡;ğŸš€+EMBRACE+THE+NEURAL+REVOLUTION+ğŸš€" alt="Footer" />
</div>

<div align="center" style="margin: 20px 0;">
  <img src="https://komarev.com/ghpvc/?username=HKUDS&repo=RAG-Anything&color=FF0080&style=for-the-badge&label=NEURAL+VISITORS" alt="Visitor Count" />
</div>
